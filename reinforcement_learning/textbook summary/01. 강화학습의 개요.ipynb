{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. 강화학습의 개요"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 에이전트 : 강화학습을 통해 스스로 학습하는 컴퓨터\n",
    "- 에이전트는 환경에 대해 사전지식이 없는 상태에서 학습\n",
    "- 에이전트는 자신이 놓인 환경에서 자신의 상태를 인식한 후, 행동\n",
    "- 환경은 에이전트에게 보상을 주고 다음 상태를 알려줌 -> 어떤 행동이 좋은 행동인지 간접적으로 알려줌\n",
    "\n",
    "![image](http://thevoid.ghost.io/content/images/2016/06/suttonbarto_rl.png)\n",
    "\n",
    "### 강화학습의 목적 : 에이전트가 환경을 탐색하면서 얻는 보상들의 합을 최대화하는 '최적의 행동양식, 정책'을 학습하는 것"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 강화학습의 장점\n",
    "- 환경에 대한 사전지식이 없어도 학습\n",
    "- 하지만 이는 초반의 느린 학습의 원인이 됨\n",
    "- ex) 우리가 코딩을 처음 배울때, print('hello world')를 하며 아! 이러면 출력이 되는구나! 이런 것들도 강화학습으로 볼 수 있을 것 같네요\n",
    "\n",
    "## 강화학습을 적용할 문제\n",
    "- 현재 위치에서 행동을 한 번 선택하는 것이 아니라 계속적으로 선택해야 합니다. 이는 다이내믹 프로그래밍에 적용할 수 있고, 진화 알고리즘에도 적용 가능\n",
    "\n",
    "### 순차적 행동 결정 문제\n",
    "- 순차적으로 행동을 결정하는 문제를 '수학적'으로 정의해야 함 => MDP ( Markov Decision Process )\n",
    "\n",
    "## 순차적 행동 결정 문제의 구성 요소\n",
    "### 1. 상태 ( state )\n",
    "- 현재 에이전트의 정보 ( 정적인 요소 )\n",
    "- 에이전트가 움직이는 속도 ( 동적인 요소 )\n",
    "- 엄밀히 말하면 상태보다는 '관찰'이 정확한 표현\n",
    "\n",
    "### 2. 행동 ( action )\n",
    "- 에이전트가 어떠한 상태에서 취할 수 있는 행동으로 '상','하','좌','우'같은 것들을 의미\n",
    "- 어떤 행동이 좋은 행동인지 정보가 없는 경우(학습이 되지 않은 상태)엔 무작위로 행동을 취하며 특정한 행동들을 할 확률을 높임\n",
    "\n",
    "### 3. 보상 ( reward )\n",
    "- 에이전트가 학습할 수 있는 유일한 정보\n",
    "- 보상을 통해 자신의 행동을 평가하고 어떤 행동이 더 좋은지 알 수 있음\n",
    "\n",
    "### 4. 정책 ( policy )\n",
    "- 모든 상태에 대해 에이전트가 어떤 행동을 해야 하는지 정해놓은 것\n",
    "- 제일 좋은 정책은 최적 정책 ( optimal policy )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 강화학습의 예시 : 브레이크아웃\n",
    "## 아타리 게임\n",
    "- 강화학습은 에이전트가 직접 환경과 상호작용하면서 보상을 최대화하도록 정책을 찾는 과정\n",
    "- 딥마인드의 \"Playing Atari with Deep Reinforcement Learing\" 논문 Check!\n",
    "### 1. MDP\n",
    "    - 상태 : 게임 화면\n",
    "        - 연속으로 화면 4개를 받고, 하나의 상태로 에이전트에게 제공 ( 2차원 픽셀 데이터 )\n",
    "    - 행동 \n",
    "        - 제자리, 왼쪽, 오른쪽, 발사가 가능하고 브레이크 아웃에서 발사는 게임을 시작할 때 사용\n",
    "        - 한 상태에서 행동을 결정하면 잠깐 동안 행동을 반복 ( 4개의 화면에선 같은 행동을 해야 학습 가능! )\n",
    "    - 보상\n",
    "        - 벽돌이 하나 깨질 때마다, 보상을 (+1)씩 받고 더 위쪽을 깰수록 더 큰 보상\n",
    "        - 아무것도 깨지 않을 때는 보상으로 (0)을 받음\n",
    "        - 공을 놓쳐서 목숨을 잃을 경우엔 보상으로 (-1)을 받음\n",
    "### 2. 학습\n",
    "    - 게임 화면에서 000이라고 되어 있는 것은 누적된 보상을 의미\n",
    "    - 강화학습을 통해 학습되는 것은 인공신경망\n",
    "    - 행동이 얼마나 좋은지가 행동의 가치가 되고 이것을 큐 함수 ( Q function )라고 함\n",
    "    - 이 문제에서 사용한 인공신경망은 DQN (Deep Q-Network)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
