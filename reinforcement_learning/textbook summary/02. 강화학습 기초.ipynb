{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 02. 강화학습 기초\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 강화학습이 결국 어떠한 방정식을 풀어내는 방법이라면 그 방정식이 무엇인지 아는 것이 중요할 것입니다\n",
    "- 이 방정식은 벨만 방정식을 가리킵니다\n",
    "\n",
    "## MDP\n",
    "- 구성 요소 : 상태 / 행동 / 보상 함수 / 상태 변환 확률 / 감가율\n",
    "- 강화학습에서는 사용자가 문제를 정의해야 합니다\n",
    "\n",
    "### 상태\n",
    "- 상태 S는 에이전트가 관찰 가능한 상태의 집합\n",
    "- 내가 정의하는 상태는 에이전트가 학습하기에 충분한 정보일까?란 생각을 곱씹어봐야함\n",
    "- S = {(x1,y1), (x2,y2), (x3,y3), (x4,y4),(x5,y5)} 이런 식!\n",
    "\n",
    "![image](http://thevoid.ghost.io/content/images/2016/06/maze1.png)\n",
    "(위 사진은 참고를 위해 가져왔을 뿐, 설명과 상관 없음)\n",
    "\n",
    "- 5 x 5 그리드월드에서 S = {(1,1),(1,2),(1,3)....(5,5)}\n",
    "- 에이전트는 25개의 상태집합 안에 있는 상태를 탐험하게 됩니다. 시간은 t일 때 상태를 S_t로 표현합니다! 예를 들면 아래와 같음"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{array}{c}\n",
    "S_t = (1,3)\n",
    "\\end{array}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "t= 1일때 S_t = (1,3)일 수도 있고, (4,2)일 수도 있음. 이것들은 확률 변수 ( Random Variable )라고 하며, \n",
    "\n",
    "시간 t에서의 상태 S_t가 어떤 상태 s다를 수식으로 표현하면 아래와 같습니다"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{array}{c}\n",
    "S_t = s\n",
    "\\end{array}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 행동\n",
    "- 에이전트가 상태 S에서 할 수 있는 가능한 행동의 집합은 A입니다. \n",
    "- 보통 에이전트가 할 수 있는 행동은 모든 상태에서 같습니다\n",
    "- 시간 t에 에이전트가 특정한 행동 a를 했다면 아래와 같이 표현할 수 있습니다"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{array}{c}\n",
    "A_t = a\n",
    "\\end{array}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{array}{c}\n",
    "A_t\n",
    "\\end{array} 는 어떤 t라는 시간에 집합 A에서 선택한 행동입니다. 이 친구도 확률변수\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A = {up, down, left, right}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 보상 함수\n",
    "- 에이전트가 학습할 수 있는 유일한 정보로 환경이 에이전트에게 주는 정보\n",
    "- 보상함수 : 시간 t일 때 상태가 s이고 그 상태에서 행동 a를 했을 경우 받을 보상에 대한 기댓값 E\n",
    "- 아래와 같은 수식으로 정의합니다\n",
    "- 조건부 확률, 기댓값"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{array}{c}\n",
    "R_s^a = E[R_t+1|S_t=s, A_t=a]\n",
    "\\end{array}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 어떤 상태에서 행동한 것은 시간 t인데, 보상을 받는 것은 t+1\n",
    "- 보상을 에이전트가 알고 있는 것이 아니고 환경이 알려주기 떄문에 위와같은 수식이 나온 것"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 상태 변환 확률\n",
    "- 에이전트가 앞으로 나아가는 행동을 하면 s보다 앞에 있는 s'라는 상태에 도달할 것입니다\n",
    "- 하지만 넘어진다거나 바람이 불면 상태가 변환할 수 있는데, 이를 수치적으로 표현한 것"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{array}{c}\n",
    "P^a_{ss'} = P[S_t+1=s'|S_t=s, A_t=a]\n",
    "\\end{array}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 상태 변환 확률 : 상태 s에서 행동 a를 취했을 떄 다른 상태 s'에 도달할 확률\n",
    "- 에이전트가 아닌 환경의 일부이며, 환경의 모델이라고 부릅니다"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 감가율\n",
    "- 에이전트는 항상 현재에 판단을 내리기 때문에 현재에 가까운 보상일수록 더 큰 가치를 가짐\n",
    "- 현재로부터 일정 시간이 흐른 후, 보상을 받으면 100이라고 하지 않고 감가시켜 현재의 가치로 따짐"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{array}{c}\n",
    "\\gamma \\in [0, 1]\n",
    "\\end{array}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 현재의 시간 t로부터 시간 k가 지난 후에 보상을 R_t+k를 받을 것이라면 아래와 같은 수식으로 표현\n",
    "$$\n",
    "\\begin{array}{c}\n",
    "\\gamma^{k-1}R_{t+k}\n",
    "\\end{array}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 정책\n",
    "- 정책은 모든 상태에서 에이전트가 할 행동\n",
    "- 상태가 입력으로 들어오면 행동을 출력으로 내보내는 일종의 함수"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{array}{c}\n",
    "\\pi(a|s) = P[A_t=a|S_t=s]\n",
    "\\end{array}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 최적 정책을 얻기 위해 현재의 정책보다 더 좋은 정책을 학습해나가는 것이 강화학습\n",
    "- 환경은 에이전트에게 실제 보상과 상태를 알려줍니다. 이 과정에서 예상한 보상에 대해 틀렸다는 것을 알게 됩니다\n",
    "- 이 때 받을 것이라 예상하는 보상을 가치함수라 합니다"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 가치함수\n",
    "- MDP -> 가치함수 -> 행동 선택\n",
    "- 현재 시간 t로부터 에이전트가 행동을 하면서 받을 보상들을 합한다면 아래와 같습니다\n",
    "$$\n",
    "\\begin{array}{c}\n",
    "R_{t+1}+R_{t+2}+R_{t+3}+R_{t+4}+...\n",
    "\\end{array}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 에이전트가 시간마다 보상을 받을 수도 있고 게임이 끝날 때 한 번에 받을 수도 있습니다\n",
    "- 감가율을 고려한 보상들의 합은(반환값)\n",
    "$$\n",
    "\\begin{array}{c}\n",
    "G_t = R_{t+1}+\\gamma R_{t+2}+\\gamma^2 R_{t+3}+\\gamma^3 R_{t+4}+...\n",
    "\\end{array}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 어떠한 상태에 있으면 앞으로 얼마의 보상을 받을 것인지에 대한 기댓값을 고려해볼 수 있는데, 이것이 바로 가치함수입니다\n",
    "$$\n",
    "\\begin{array}{c}\n",
    "v(s) = E[G_t|S_t=s]\n",
    "\\end{array}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 각 스텝마다 받는 보상이 모두 확률적이고 반환값이 그 보상들의 합이므로 반환값은 확률변수입니다\n",
    "- 하지만 가치함수는 확률변수가 아니라 특정 양을 나타내는 값으므로 소문자로 표현 ( 에이전트가 가지고 있는 값! )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{array}{c}\n",
    "v(s) = E[R_{t+1}+\\gamma R_{t+2}+\\gamma^2 R_{t+3}+\\gamma^3 R_{t+4}+...|S_t=s]\n",
    "\\end{array}\n",
    "$$\n",
    "이를 감마로 묶어주고 반환값의 형태로 표현하면\n",
    "$$\n",
    "\\begin{array}{c}\n",
    "v(s) = E[R_{t+1}+\\gamma G_{t+1}|S_t=s]\n",
    "\\end{array}\n",
    "$$\n",
    "이 보상은 앞으로 받을 것이라 예상하는 보상이기에 가치함수로 표현해보면\n",
    "$$\n",
    "\\begin{array}{c}\n",
    "v(s) = E[R_{t+1}+\\gamma v(S_{t_1})|S_t=s]\n",
    "\\end{array}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 벨만 기대 방정식 ( Bellman Expectation Equation )\n",
    "- 정책을 고려한 가치함수의 표현 \n",
    "- 현재 상태의 가치 함수와 다음 상태의 가치함수 사이의 관계를 말해주는 방정식\n",
    "$$\n",
    "\\begin{array}{c}\n",
    "v_\\pi(s) = E_\\pi[R_{t+1}+\\gamma v_\\pi(S_{t_1})|S_t=s]\n",
    "\\end{array}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
